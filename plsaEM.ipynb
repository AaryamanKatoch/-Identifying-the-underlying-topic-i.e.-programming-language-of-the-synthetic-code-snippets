{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRX_B3DGoK4J"
      },
      "source": [
        "# Overview \n",
        "\n",
        "This project is to implement probabilistic Latent Semantic Analysis\n",
        " (pLSA) and its computation - expectation maximization (EM) algorithm.  pLSA will help identify the underlying topic (i.e., programming language) of the synthetic code snippets. \n",
        "\n",
        " pipeline:\n",
        "\n",
        "- Preparation:\n",
        "    - Downloading the dataset and loading required libraries.\n",
        "    - Inspecting data.\n",
        "-Implementating pLSA based on EM algorithm with unlabeled data only; \n",
        "-Improving your implementation in Part 1 by incorporating labeled data; .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Library requirement**:  **NOT** using existing libraries that have already implemented pLSA algorithms ; \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX488frNqSmE"
      },
      "source": [
        "# Loading Dataset and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fbO8PghlT03W"
      },
      "outputs": [],
      "source": [
        "! rm -rf cs589*\n",
        "! rm -rf result*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p078QiaEqbT8"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import trange\n",
        "from collections import Counter\n",
        "from cs589.utils.common import (   \n",
        "    seed_everything,\n",
        "    post_processing\n",
        ")\n",
        "\n",
        "seed_everything(0)\n",
        "\n",
        "pd.options.display.max_columns = 10\n",
        "pd.options.display.max_colwidth = 20\n",
        "\n",
        "save_path = pathlib.Path(\"result\")\n",
        "if not save_path.exists(): \n",
        "    save_path.mkdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isawdB3LvvJR"
      },
      "source": [
        "# Looking at the Data\n",
        "\n",
        "The pLSA considers a generatative process. For each $i$-th position of the document $d$ is determined by two distributions:\n",
        "1. First choose a topic $z_i$:\n",
        "$$z_i \\sim \\mathrm{Multinomial}(\\theta_d)$$\n",
        "where $\\theta_d$ is the document-topic vector of document $d$; its length is the number of topics of a corpus; this makes $\\theta_d \\in \\mathbb{R}^2$.\n",
        "2. Then choose a term $w_i$ under the topic in the first step: \n",
        "$$w_i \\sim \\mathrm{Multinomial}(\\phi_z)$$\n",
        "where $\\phi_z$ is the topic-word vector for each topic; its length is the number of terms of a topic; this makes $\\phi_z \\in \\mathbb{R}^8$.\n",
        "\n",
        "In this assignment,\n",
        "- Our corpus has 2 topics, i.e., Java and Python.\n",
        "- Our corpus has 8 possible words (i.e., the vocabulary `vocab`); they are all [reserved words](https://en.wikipedia.org/wiki/Reserved_word) for the Java and Python programming languages\n",
        "```python\n",
        "vocab = {\n",
        "    \"import\", \"class\", \"arraylist\", \"hashset\"\n",
        "    \"hashmap\", \"lambda\", \"def\", \"elif\"\n",
        "}\n",
        "```\n",
        "\n",
        "As a developer, we can tell a program is written in Python if it includes `def`, `lambda`, and `elif`; a program is written in Java if it has `arraylist`, `hastset`, and `hashmap`; while `import` and `class` does not provide a lot of information for the underlying language as both languages share them. \n",
        "\n",
        "In this project, we need to teach the computer to learn about this through pLSA. Let's first look at the data; we have \n",
        "- 100 samples that have known topics (Java or Python).\n",
        "- 10000 samples whose topics are waiting to be discovered.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LkLQiVetoGKp"
      },
      "outputs": [],
      "source": [
        "n_topic = 2\n",
        "n_word = None\n",
        "n_doc = None\n",
        "n_labeled = None\n",
        "\n",
        "df = pd.read_csv(\"cs589/dataset/dataset.txt\", \n",
        "                 sep=\"\\t\", \n",
        "                 names=[\"label\", \"text\"]).convert_dtypes(int)\n",
        "\n",
        "vocab = set(itertools.chain(*[text.split() for text in df.text.tolist()]))\n",
        "\n",
        "labeled_df = df[~df.label.isna()]\n",
        "unlabeled_df = df[df.label.isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx3-60W4rKxp",
        "outputId": "aa0e9cea-a562-477e-be63-ad0d492d211b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   label                 text\n",
            "0      0  arraylist class ...\n",
            "1      1  lambda class lam...\n",
            "2      1  lambda lambda el...\n",
            "3      0  hashmap hashmap ...\n",
            "4      1  import elif def ...\n",
            "     label                 text\n",
            "100   <NA>  hashmap import h...\n",
            "101   <NA>  class lambda has...\n",
            "102   <NA>  import class imp...\n",
            "103   <NA>  arraylist hashma...\n",
            "104   <NA>  def elif import ...\n"
          ]
        }
      ],
      "source": [
        "print(labeled_df.head())\n",
        "print(unlabeled_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_bb3_gRY22fo"
      },
      "outputs": [],
      "source": [
        "token_to_index_dict = {token: index for index, token in enumerate(vocab)}\n",
        "index_to_token_dict = {v: k for k, v in token_to_index_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1qHygaVaaKAr"
      },
      "outputs": [],
      "source": [
        "# helper function\n",
        "def format_result(unlabeled_df, topic_word_mat, document_topic_mat):\n",
        "    topic_word_df = pd.DataFrame(\n",
        "        topic_word_mat,\n",
        "        columns=list(token_to_index_dict.keys()),\n",
        "        index=[\"Java\", \"Python\"]\n",
        "        )\n",
        "\n",
        "    document_topic_df = pd.DataFrame(document_topic_mat, columns=[\"Java\", \"Python\"])\n",
        "    document_topic_df = document_topic_df.assign(text=unlabeled_df.text.tolist())\n",
        "\n",
        "    return topic_word_df, document_topic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8Vi1y5b2W4c-"
      },
      "outputs": [],
      "source": [
        "n_word = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtUA9i5yC5sK"
      },
      "source": [
        "The code snippet below represents each code snippet as vector of size 8, where each entry is the word count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "71jVJyIU2x7p"
      },
      "outputs": [],
      "source": [
        "def get_token_cnt_vector(text):\n",
        "    # this function depends on n_word and token_to_index_dict variables\n",
        "    # initialized above\n",
        "\n",
        "    token_cnt_vector = np.zeros(n_word)\n",
        "\n",
        "    for token, cnt in Counter(text.split()).items():\n",
        "        index = token_to_index_dict[token]\n",
        "        token_cnt_vector[index] = cnt\n",
        "    \n",
        "    return token_cnt_vector\n",
        "\n",
        "labeled_doc_mat = np.vstack(labeled_df.text.apply(get_token_cnt_vector).tolist())\n",
        "unlabeled_doc_mat = np.vstack(unlabeled_df.text.apply(get_token_cnt_vector).tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlnYezPYDca3"
      },
      "source": [
        "For example, let's look at the first labeled document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJYF0fAIDi10",
        "outputId": "767017c4-34af-4842-94cc-c2a1deb55c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "arraylist class hashmap hashmap arraylist hashmap hashmap hashset hashmap hashmap hashmap hashmap import hashmap class import class hashmap import hashmap hashmap arraylist hashmap hashmap arraylist hashset hashmap hashmap hashset class hashmap import arraylist hashmap arraylist hashmap hashset hashmap hashset hashmap import arraylist arraylist arraylist hashset hashmap import hashset hashset hashset import hashset class hashset hashmap hashset hashmap hashmap import class arraylist hashmap hashset hashset hashset arraylist hashmap import arraylist hashmap arraylist import arraylist hashmap import hashset class import hashmap hashmap class hashset class hashmap hashset hashset import hashmap class hashmap hashmap class arraylist hashmap hashmap import hashset hashmap hashmap class \n",
            "[15. 20. 12. 14.  0.  0. 39.  0.]\n"
          ]
        }
      ],
      "source": [
        "print(labeled_df.label.tolist()[0])\n",
        "print(labeled_df.text.tolist()[0])\n",
        "print(labeled_doc_mat[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPXqTVmeDPrg"
      },
      "source": [
        "We have 100 labeled documents and 10000 unlabeled ones; this gives us two matrices of shape `(100, 8)` and `(10000, 8)` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSo_BeJh2799",
        "outputId": "70f35524-7056-484c-8646-e144b3fd2edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 8)\n",
            "(10000, 8)\n"
          ]
        }
      ],
      "source": [
        "print(labeled_doc_mat.shape)\n",
        "print(unlabeled_doc_mat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUDutKqCHGa"
      },
      "source": [
        "\n",
        "- Part 1: Estimate $\\theta_d$ and $\\phi_z$ with unlabeled data only using pLSA.\n",
        "- Part 2: Use the labeled data to improve the estimation in step 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOi0-utCcUu"
      },
      "source": [
        "# Part 1: pLSA with Unlabeled Data\n",
        "\n",
        "The E-step and M-step are shown below:\n",
        "\n",
        "## E-Step\n",
        "\n",
        "For each document $j=1,2,\\cdots, 10000$, each word $i=1,2,\\cdots,8$, and each topic $k=1,2$:\n",
        "- At the 1st iteration, computing $p(z_k\\vert w_i, d_j)$ depends on randomly initialized $p(w_i\\vert z_k)$ and $p(z_k\\vert d_j)$.\n",
        "- At the rest of iterations, computing $p(z_k\\vert w_i, d_j)$ depends on $p(w_i\\vert z_k)$ and $p(z_k\\vert d_j)$ computed from the last iteration.\n",
        "\n",
        "$$\n",
        "p(z_k\\vert w_i, d_j) = \n",
        "\\frac{p(w_i\\vert z_k) p(z_k\\vert d_j)}{\\sum_{k=1}^2p(w_i\\vert z_k)p(z_k\\vert d_j)}\n",
        "$$\n",
        "\n",
        "## M-Step\n",
        "- For each document $j=1,2,\\cdots,10000$, each word $i=1,2,\\cdots,8$, and each topic $k=1,2$:\n",
        "$$\n",
        "p(w_i\\vert z_k)=\n",
        "\\frac{\\sum_{j=1}^{10000} n(w_i, d_j) p(z_k\\vert w_i, d_j)}{\\sum_{m=1}^8\\sum_{j=1}^{10000} n(w_m, d_j) p(z_k\\vert w_m, d_j)}\n",
        "$$\n",
        "where $n(w_m, d_j)$ is the number of appearances of the word $w_m$ in the document $d_j$.\n",
        "- For each document $j=1,2,\\cdots, 10000$, each word $i=1,2,\\cdots, 8$, and each topic $k=1,2$:\n",
        "$$\n",
        "p(z_k\\vert d_j)=\n",
        "\\frac{\\sum_{i=1}^{10000} n(w_i, d_j) p(z_k\\vert w_i, d_j)}{\\sum_{i=1}^{10000} n(w_i, d_j)}\n",
        "$$\n",
        "where $n(w_i, d_j)$ is the number of appearances of the word $w_i$ in the document $d_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6bG8TPlCbWs"
      },
      "source": [
        "In code, the $\\theta_d$ and $\\phi_z$ are represented as `document_topic_mat` and `topic_word_mat` for efficient computation:\n",
        "\n",
        "\n",
        "- `document_topic_mat`:\n",
        "$$\n",
        "    \\begin{pmatrix}\n",
        "    - &\\theta_1^T& - \\\\\n",
        "    - &\\vdots &- \\\\\n",
        "    - &\\theta_{10000}^T& - \\\\\n",
        "    \\end{pmatrix}\n",
        "    \\in \\mathbb{R}^{10000\\times 2} \n",
        "$$\n",
        "\n",
        "- `topic_word_mat`: It is a matrix representing the topic word distribution of the **entire corpus**.\n",
        "\n",
        "$$\n",
        "    \\begin{pmatrix}\n",
        "    -&\\phi_\\mathrm{Java}^{T}&-\\\\\n",
        "    -&\\phi_\\mathrm{Python}^{T}&-\n",
        "    \\end{pmatrix}\n",
        "    \\in \\mathbb{R}^{2 \\times 8}\n",
        "$$\n",
        "\n",
        "Another variable you could see in the code is `pred`:\n",
        "\n",
        "- `pred`: It is a matrix stacked with 10000 matrices, each representing the topic word distribution of **each document**.\n",
        "\n",
        "$$\n",
        "    \\begin{pmatrix}\n",
        "    \\begin{pmatrix}\n",
        "    - &\\phi_\\mathrm{Java}^{(1)T}&-\\\\\n",
        "    -&\\phi_\\mathrm{Python}^{(1)T}&-\n",
        "    \\end{pmatrix}\\\\\n",
        "    \\vdots\\\\\n",
        "    \\begin{pmatrix}\n",
        "    -&\\phi_\\mathrm{Java}^{(10000)T}&-\\\\\n",
        "    -&\\phi_\\mathrm{Python}^{(10000)T}&-\n",
        "    \\end{pmatrix}\n",
        "    \\end{pmatrix} \\in \\mathbb{R}^{10000 \\times 2 \\times 8}\n",
        "$$\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m6TqB9Euzyci"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "n_word = len(vocab)\n",
        "n_labeled = len(labeled_df)\n",
        "n_doc = len(unlabeled_df)\n",
        "n_iteration = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGJkMdJU0EJz"
      },
      "source": [
        "## E-Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q4hdWBTWu1eh"
      },
      "outputs": [],
      "source": [
        "def E_step(document_topic_mat, topic_word_mat):   \n",
        "    \"\"\"\n",
        "    Input:\n",
        "        document_topic_mat: (n_doc, n_topic), probability of each document being one of the topics\n",
        "        topic_word_mat: (n_topic, n_word), probability of each topic has each word for the entire corpus\n",
        "    \n",
        "    Output:\n",
        "        pred: (n_doc, n_word, n_topic), probability of each topic has each for for each document\n",
        "    \"\"\"\n",
        "\n",
        "    pred = np.zeros((n_doc, n_word, n_topic))\n",
        "\n",
        "\n",
        "    for j in range(0, n_doc):\n",
        "        for i in range(0, 8):\n",
        "            denominator = 0;\n",
        "            for k in range(0, 2):\n",
        "                pred[j,i,k] = document_topic_mat[j,k] * topic_word_mat[k,i];\n",
        "                denominator += pred[j,i,k];\n",
        "            if denominator == 0:\n",
        "                for k in range(0, 2):\n",
        "                    pred[j,i,k] = 0;\n",
        "            else:\n",
        "                for k in range(0, 2):\n",
        "                    pred[j,i,k] /= denominator;\n",
        "\n",
        "    \n",
        "    ##############################################END HERE##############################################\n",
        "    \n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEmBddra0Itk"
      },
      "source": [
        "## M-Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4_84_s_Ox0vC"
      },
      "outputs": [],
      "source": [
        "def compute_document_topic_mat(pred, unlabeled_doc_mat):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        pred: (n_doc, n_word, n_topic), probability of each topic has each for for each document\n",
        "        unlabeled_doc_mat: (n_doc, n_word), word count for each document\n",
        "    \n",
        "    Output:\n",
        "        document_topic_mat: (n_doc, n_topic), probability of each document being one of the topics\n",
        "    \"\"\"\n",
        "\n",
        "    document_topic_mat = np.zeros((n_doc, n_topic))\n",
        "    for j in range(n_doc):\n",
        "        for v in range(n_word):\n",
        "            document_topic_mat[j][0] += pred[j][v][0] * unlabeled_doc_mat[j][v]\n",
        "            document_topic_mat[j][1] += pred[j][v][1] * unlabeled_doc_mat[j][v]\n",
        "    \n",
        "\n",
        "    return document_topic_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GCBfI8tcydh5"
      },
      "outputs": [],
      "source": [
        "def compute_topic_word_mat(pred, unlabeled_doc_mat):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        pred: (n_doc, n_word, n_topic), probability of each topic has each for for each document\n",
        "        unlabeled_doc_mat: (n_doc, n_word), word count for each document\n",
        "    \n",
        "    Output:\n",
        "        topic_word_mat: (n_topic, n_word), probability of each topic has each word for the entire corpus\n",
        "    \"\"\"\n",
        "    topic_word_mat = np.zeros((n_topic, n_word))\n",
        "\n",
        "    for k in range(0, 2):\n",
        "       denominator = 0\n",
        "       for i in range(0, 8):\n",
        "         topic_word_mat[k,i] = 0\n",
        "         for j in range(0, 10000):\n",
        "           topic_word_mat[k,i] += unlabeled_doc_mat[j, i] * pred[j, i, k]\n",
        "         denominator += topic_word_mat[k,i]\n",
        "       if denominator == 0:\n",
        "           for i in range(0, 8):\n",
        "                topic_word_mat[k,i] = 1.0 / 8\n",
        "       else:\n",
        "            for i in range(0, 8):\n",
        "                topic_word_mat[k,i] /= denominator\n",
        "    \n",
        "    \n",
        "    ##############################################END HERE################################################\n",
        "    \n",
        "\n",
        "    return topic_word_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FrkMr4CCvba1"
      },
      "outputs": [],
      "source": [
        "def M_step(pred, unlabeled_doc_mat):\n",
        "    document_topic_mat = compute_document_topic_mat(pred, unlabeled_doc_mat)\n",
        "    topic_word_mat = compute_topic_word_mat(pred, unlabeled_doc_mat)\n",
        "\n",
        "    document_topic_mat /= np.sum(document_topic_mat, axis=1, keepdims=True)\n",
        "    topic_word_mat /= np.sum(topic_word_mat, axis=1, keepdims=True)\n",
        "    \n",
        "    return document_topic_mat, topic_word_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CDkoMul0McQ"
      },
      "source": [
        "## EM Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cuSTkS5krIT1"
      },
      "outputs": [],
      "source": [
        "def compute_plsa_with_unlabaled_data(unlabeled_doc_mat):\n",
        "    seed_everything(0)\n",
        "\n",
        "    # initialize topic_word_mat\n",
        "    topic_word_mat = np.random.rand(n_topic, n_word)\n",
        "    topic_word_mat /= np.sum(topic_word_mat, axis=1, keepdims=True)\n",
        "\n",
        "    # initialize document_topic_mat\n",
        "    document_topic_mat = np.random.rand(n_doc, n_topic)\n",
        "    document_topic_mat /= np.sum(document_topic_mat, axis=1, keepdims=True)\n",
        "\n",
        "    pred = np.zeros((n_doc, n_word, n_topic))\n",
        "\n",
        "    for _ in trange(n_iteration):\n",
        "        # E-step\n",
        "        pred = E_step(document_topic_mat, topic_word_mat)\n",
        "\n",
        "        # M-step\n",
        "        document_topic_mat, topic_word_mat = M_step(pred, unlabeled_doc_mat)\n",
        "    \n",
        "    return document_topic_mat, topic_word_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2KpQFN0ZDig",
        "outputId": "9f3cde7f-26ba-4d2c-ef25-3a43a31439c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:00<00:00,  1.66it/s]\n"
          ]
        }
      ],
      "source": [
        "document_topic_mat, topic_word_mat = compute_plsa_with_unlabaled_data(unlabeled_doc_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m7INe0C_Zgh2"
      },
      "outputs": [],
      "source": [
        "topic_word_df, document_topic_df = format_result(unlabeled_df, topic_word_mat, document_topic_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gXePcLGcL6s"
      },
      "source": [
        "You can now check if your implementation is correct by looking at:\n",
        "\n",
        "- `topic_word_df`: If word (i.e., reserved words of each programming langauage) probabilities match the actual language each word belongs to.\n",
        "- `document_topic_df`: If topic (i.e., programming languages) probabilities show the actual language the code snippet is written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZvqeE2OcM-F",
        "outputId": "2a1b5e2e-a325-4dcc-bcc2-49a0b085809c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        arraylist   hashset     class    import       def      elif   hashmap  \\\n",
            "Java     0.003812  0.003258  0.101324  0.100141  0.196975  0.196832  0.007385   \n",
            "Python   0.199838  0.199023  0.099518  0.099762  0.001215  0.001495  0.395571   \n",
            "\n",
            "          lambda  \n",
            "Java    0.390273  \n",
            "Python  0.003578  \n",
            "       Java    Python                 text\n",
            "0  0.487521  0.512479  hashmap import h...\n",
            "1  0.659319  0.340681  class lambda has...\n",
            "2  0.729905  0.270095  import class imp...\n",
            "3  0.395617  0.604383  arraylist hashma...\n",
            "4  0.841896  0.158104  def elif import ...\n"
          ]
        }
      ],
      "source": [
        "print(topic_word_df)\n",
        "print(document_topic_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "znSS9Q6zdeDx"
      },
      "outputs": [],
      "source": [
        "# save results for submission\n",
        "topic_word_df.to_pickle(save_path / \"unlabeled_topic_word.pkl\")\n",
        "document_topic_df.to_pickle(save_path / \"unlabeled_document_topic.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAD452khCiKE"
      },
      "source": [
        "# Part 2: pLSA with Labeled and Unlabeled Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "587D8rY-ZvqN"
      },
      "source": [
        "The estimation of `topic_word_mat` in Part 1 could be improved with our labeled data: on top of the regular M-step, the only change is that instead of using estimated `pred[j][i][k]`, we could known label `label[j][k]` from  each labeled document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdIPVlCvTLn5"
      },
      "source": [
        "## Enhanced M-Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9Xcr3y9IUzuG"
      },
      "outputs": [],
      "source": [
        "# global variables\n",
        "label = np.zeros((n_labeled, n_topic))\n",
        "for idx, topic in enumerate(labeled_df.label.tolist()): \n",
        "    label[idx, topic] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TLmBl0rjxRFD"
      },
      "outputs": [],
      "source": [
        "def enhanced_M_step(pred, labeled_doc_mat, unlabeled_doc_mat):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        pred: (n_doc, n_word, n_topic), probability of each topic has each for for each document\n",
        "        labeled_doc_mat: (n_labeled, n_word), word count for each labeled document\n",
        "        unlabeled_doc_mat: (n_doc, n_word), word count for each unlabeled document\n",
        "    \n",
        "    Output:\n",
        "        document_topic_mat: (n_doc, n_topic), probability of each document being one of the topics\n",
        "        topic_word_mat: (n_topic, n_word), probability of each topic has each word for the entire corpus\n",
        "    \"\"\"\n",
        "\n",
        "    document_topic_mat = compute_document_topic_mat(pred, unlabeled_doc_mat)\n",
        "    topic_word_mat = compute_topic_word_mat(pred, unlabeled_doc_mat)\n",
        "\n",
        "\n",
        "    for k in range(0, 2):\n",
        "       denominator = 0\n",
        "       for i in range(0, 8):\n",
        "         \n",
        "         for j in range(0, len(labeled_df)):\n",
        "           topic_word_mat[k,i] += labeled_doc_mat[j,i] * label[j][k]\n",
        "         denominator += topic_word_mat[k,i]\n",
        "       if denominator == 0:\n",
        "           for i in range(0, 8):\n",
        "                topic_word_mat[k,i] = 1.0 / 8\n",
        "       else:\n",
        "            for i in range(0, 8):\n",
        "                topic_word_mat[k,i] /= denominator\n",
        "\n",
        "    ##############################################END HERE################################################\n",
        "\n",
        "    document_topic_mat /= np.sum(document_topic_mat, axis=1, keepdims=True)\n",
        "    topic_word_mat /= np.sum(topic_word_mat, axis=1, keepdims=True)\n",
        "    \n",
        "    return document_topic_mat, topic_word_mat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0UN7PWxT5BB"
      },
      "source": [
        "## EM Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "p8SI3ySc1-EG"
      },
      "outputs": [],
      "source": [
        "def compute_plsa_with_all_data(labeled_doc_mat, unlabeled_doc_mat):\n",
        "    seed_everything(0)\n",
        "\n",
        "    # initialize topic_word_mat\n",
        "    topic_word_mat = np.random.rand(n_topic, n_word)\n",
        "    topic_word_mat /= np.sum(topic_word_mat, axis=1, keepdims=True)\n",
        "\n",
        "    # initialize document_topic_mat\n",
        "    document_topic_mat = np.random.rand(n_doc, n_topic)\n",
        "    document_topic_mat /= np.sum(document_topic_mat, axis=1, keepdims=True)\n",
        "\n",
        "    pred = np.zeros((n_doc, n_word, n_topic))\n",
        "\n",
        "    for _ in trange(n_iteration):\n",
        "        # E-step\n",
        "        pred = E_step(document_topic_mat, topic_word_mat)\n",
        "\n",
        "        # M-step\n",
        "        document_topic_mat, topic_word_mat = enhanced_M_step(pred, labeled_doc_mat, unlabeled_doc_mat)\n",
        "    \n",
        "    return document_topic_mat, topic_word_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QnM4W6EehhY"
      },
      "source": [
        "Repeat what we have done in Part 1 to check the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rfz-gWCeWqO",
        "outputId": "a9433fd4-bfa7-4155-c548-9cd4404495f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
          ]
        }
      ],
      "source": [
        "document_topic_mat, topic_word_mat = compute_plsa_with_all_data(labeled_doc_mat, unlabeled_doc_mat)\n",
        "topic_word_df, document_topic_df = format_result(unlabeled_df, topic_word_mat, document_topic_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CO9gtAlumaac",
        "outputId": "edf1f38e-9ca0-47fd-db86-f837ce5c84cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        arraylist   hashset     class    import       def      elif   hashmap  \\\n",
            "Java     0.212293  0.189838  0.100655  0.097050  0.000000  0.000000  0.400164   \n",
            "Python   0.000000  0.000000  0.107435  0.098718  0.182312  0.203589  0.000000   \n",
            "\n",
            "          lambda  \n",
            "Java    0.000000  \n",
            "Python  0.407946  \n",
            "       Java    Python                 text\n",
            "0  0.515315  0.484685  hashmap import h...\n",
            "1  0.347817  0.652183  class lambda has...\n",
            "2  0.279137  0.720863  import class imp...\n",
            "3  0.605538  0.394462  arraylist hashma...\n",
            "4  0.171616  0.828384  def elif import ...\n"
          ]
        }
      ],
      "source": [
        "print(topic_word_df)\n",
        "print(document_topic_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4F4CLnUberjC"
      },
      "outputs": [],
      "source": [
        "# save results for submission\n",
        "topic_word_df.to_pickle(save_path / \"labeled_topic_word.pkl\")\n",
        "document_topic_df.to_pickle(save_path / \"labeled_document_topic.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4p3eNyCgSU6",
        "outputId": "5bbf81cc-fd0b-4021-d176-e62cb77c8f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission\n",
            "├── labeled_document_topic.pkl\n",
            "├── labeled_topic_word.pkl\n",
            "├── unlabeled_document_topic.pkl\n",
            "└── unlabeled_topic_word.pkl\n",
            "\n",
            "0 directories, 4 files\n"
          ]
        }
      ],
      "source": [
        "! tree result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
